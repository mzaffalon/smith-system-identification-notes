\chapter{Least-Squares Estimation}
\label{chap:LS-estimation}

Consider a model
\begin{equation}
  \label{eq:LTI-with-noise}
  Y = \Phi\theta_0 + \epsilon
\end{equation}
where $Y = \begin{bmatrix}
  y_0 & \ldots && y_{N-1}
\end{bmatrix}^\top$ is a vector containing the measurements $\{y_0,\ldots, y_{N-1}\}$, $\Phi\in \mathbb{R}^{N\times p}$ is called the regressor and $\theta\in\mathbb{R}^p$ is the model parameter, the vector of $p$ unknown parameters to be estimated; $p$ is the model order.
The noise vector $\epsilon =
\begin{bmatrix}
  v_0 & \ldots & v_{N-1}
\end{bmatrix}^\top$ has zero mean $\EE{\epsilon}=0$ and covariance $\EE{\epsilon \epsilon^\top}=R$, a symmetric positive definite matrix.

The maximum likelihood (ML) probability\footnote{What is the expectation value of eq.~\eqref{eq:LS-problem}? Using eq.~\eqref{eq:thetaLS-estimate}, I \emph{think} the term $Y-\Phi\theta$ should be evaluated as
  \begin{equation*}
    Y-\Phi\thetaLS = \Phi\theta_0 + \epsilon - \Phi\theta_0 - \Phi K \Phi^\top R\epsilon = \left(I - \Phi K \Phi^\top R\right)\epsilon.
  \end{equation*}
  The average
  \begin{equation*}
    \EE{(Y - \Phi \thetaLS)^\top R^{-1} (Y - \Phi \thetaLS)} = \EE{\epsilon^\top\left(I - \Phi K \Phi^\top R\right)^\top R^{-1}\left(I - \Phi K \Phi^\top R\right)\epsilon}
  \end{equation*}
  but I do not know what the term $\EE{\epsilon^\top R^{-1}\epsilon}$ evaluates to. I expect this to be equal to $N$, at least this is what happens with an uncorrelated noise with constant variance.

  Note also that while the matrix $vv^\top$ has rank 1, the variance matrix $R$ for Gaussian distributed white noise is full rank because it is the sum of random vectors that span the whole space.} requires us to minimize
\begin{equation}
  \label{eq:LS-problem}
  (Y - \Phi \theta)^\top R^{-1} (Y - \Phi \theta) = ||C (Y - \Phi \theta)||_2^2
\end{equation}
where $C$ is the Cholesky decomposition of the symmetric positive definite matrix $C^\top C = R^{-1}$.

The minimum of eq.~\eqref{eq:LS-problem} is found by setting the gradient of the expression with respect to $\theta$ to zero, which gives the normal equation
\begin{equation*}
  \Phi^\top R^{-1}\Phi \thetaLS = \Phi^\top R^{-1}Y.
\end{equation*}
The \emph{mathematical} solution is given by
\begin{equation}
  \label{eq:LS-estimator}
  \thetaLS = \left(\Phi^\top R^{-1}\Phi\right)^{-1} \Phi^\top R^{-1}Y
\end{equation}
and it exists provided that $C\Phi$ has full rank\footnote{The usual warning holds for the rank. Instead we want to have the matrix $C\Phi$ with the smallest condition number for the estimate to be numerically stable, which is a stronger condition than full rank, to uniquely determine the best estimate $\thetaLS$.}: $\rank{C\Phi} = p$. When this is the case, the system is said to be \emph{persistently excited}, see Sect.. Given the freedom to choose $\Phi$, one must select it such that $C\Phi$ is persistently excited.

\emph{Numerically} one should not form the normal equation directly because it squares the condition number of $C\Phi$ and rely either on the QR decomposition or on the SVD to solve eq.~\eqref{eq:LS-problem}.
This is taken care automatically by MATLAB when using the backslash $\backslash$ operator
\begin{equation}
  \label{eq:LS-numerical-solution}
  \thetaLS = (C\Phi) \backslash (CY).
\end{equation}
From this, it is clear that the quantities that matter are $C\Phi$ and $CY$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bias, Covariance and MSE of the Least Squares Estimation}
\label{sec:bias-variance-MSE-LS-estimation}

The linear estimator eq.~\eqref{eq:LS-estimator} is unbiased\footnote{Recalling that $R$ and $\Phi^\top R^{-1} \Phi$ are symmetric matrices, letting $K\doteq (\Phi^\top R^{-1} \Phi)^{-1}$ and using eq.~\eqref{eq:LTI-with-noise} into eq.~\eqref{eq:LS-estimator}, we obtain
  \begin{equation}
    \label{eq:thetaLS-estimate}
    \thetaLS = K\Phi^\top R^{-1}(\Phi\theta_0 + \epsilon) = \theta_0 + K\Phi^\top R^{-1}\epsilon
  \end{equation}
  from which $\EE{\thetaLS} = \theta_0$ since $\EE{\epsilon}=0$. Moreover
  \begin{align*}
    \cov{\thetaLS} &= \EE{\left(\thetaLS - \EE{\thetaLS}\right) \left(\thetaLS - \EE{\thetaLS}\right)^\top} & \EE{\thetaLS} = \theta_0 \\
                   &= \EE{\left(K \Phi^\top R^{-1}\epsilon\right) \left(K \Phi^\top R^{-1}\epsilon\right)^\top} \\
                   &= \EE{K \Phi^\top R^{-1}\epsilon \epsilon^\top R^{-1}\Phi K} \\
                   &= K \Phi^\top R^{-1}\EE{\epsilon \epsilon^\top} R^{-1} \Phi K & \EE{\epsilon \epsilon^\top} = R \\
                   &= K.
  \end{align*}}:
\begin{equation*}
  \EE{\thetaLS} = \theta_0.
\end{equation*}
The covariance\footnote{Is there a way to understand the form of covariance matrix without going through the calculation?}
\begin{equation*}
  \cov{\thetaLS} = \left(\Phi^\top R^{-1}\Phi\right)^{-1}
\end{equation*}
Note that \emph{only} in the case of diagonal (\textit{e.g.} uncorrelate) with all elements equal $R=\sigma^2I_N$ the covariance reduces to $\sigma^2\left(\Phi^\top \Phi\right)^{-1}$. Lastly, we consider the mean squared error
\begin{equation*}
  \MSE{\thetaLS} = \underbrace{\left|\left|\Bias{\thetaLS}\right|\right|_2^2}_{=0} + \textrm{tr}\left(\cov{\thetaLS}\right)
\end{equation*}
which reduces to $\sigma^2N$ for the case of uncorrelated noise.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Geometric Interpretation of Least-Squares}

The least squares problem
\begin{equation*}
  ||b - Ax||_2
\end{equation*}
has the following geometric interpretation: the solution is that for which the residuals $v \doteq b - Ax$ are outside (\text{i.e.} orthogonal) of the space spanned by $A$.
In other words, we require\footnote{I believe the proof given in class is not correct: to span the full column space of $A$, one has to multiply by a generic vector $z$; in class $z=x$ was taken.} the scalar product $\langle Az, v\rangle$ to be zero for all $z$:
\begin{align*}
  0 &= \langle Az, v\rangle = (Az)^\top(b-Ax) = z^\top \left(A^\top b - A^\top Ax\right)\hspace{2em}\forall z \\
    &\rightarrow A^\top Ax = A^\top b
\end{align*}
which is the normal equation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Random Notes: The Covariance Matrix and the Choice of the Measurement Points}
\label{question-covariance}

These are my considerations that are not part of the lecture.

\begin{itemize}
\item The \emph{off-diagonal} elements of the covariance matrix $\cov{\theta}$ represent the correlations between the errors of the variables $\theta$.
It is therefore not justified to discard them and take $\theta$'s standard deviations as the square root of $\cov{\theta}$'s diagonal elements because one discards the correlations: the ball of probability is in general an ellipse with the axes not parallel to the variable directions.

\item Given the freedom to choose the measurement points, is there a ``best'' way of placing them?

  There are two factors that determine the covariance matrix: the choice of basis and the choice of points.
  The choice of basis is determined by the variables that one wants to extract: a linear transformation between one basis and the other will also transform the covariance matrix and the only concern may be the numerical stability (although one should expect that the measurement errors dominate).

  Determining the position of the measurement points by minimizing one (or more elements of the covariance) is in general a non-convex problem. However this is done.
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "notes"
%%% End:
