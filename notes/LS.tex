\chapter{Least-Squares Estimation}
\label{chap:LS-estimation}

A word about notation: in class we used $Y$ and $\Phi$ for the measurements and regressor matrix and $\epsilon$ for the (possibly correlated) noise. However because the noise is Gaussian, the equations below show that the terms scaled by the Cholesky decomposition $C$ of the noise correlation matrix $R$ are the relevant ones. With the scaling, the scaled noise $e$ becomes uncorrelated and has variance $\sigma^2=1$. I personally like this approach.\\

Consider the model
\begin{equation}
  \label{eq:LTI-with-noise}
  \tilde{Y} = \tilde{\Phi}\theta_0 + \tilde{\epsilon}
\end{equation}
where $\tilde{Y} = \begin{bmatrix}
  y_0 & \ldots && y_{N-1}
\end{bmatrix}^\top$ is a vector containing the measurements $\{y_0,\ldots, y_{N-1}\}$, $\tilde{\Phi}\in \mathbb{R}^{N\times p}$ is called the \emph{regressor} and $\theta\in\mathbb{R}^p$ is the model parameter to be estimated; $p$ is the model order. The noise vector $\tilde{\epsilon} =
\begin{bmatrix}
  v_0 & \ldots & v_{N-1}
\end{bmatrix}^\top$ has zero mean $\EE{\tilde{\epsilon}}=0$ and covariance\footnote{Note also that while the matrix $vv^\top$ has rank 1, the noise covariance matrix $R$ for Gaussian distributed white noise is full rank because it is the sum of random vectors that span the whole space.} $\EE{\tilde{\epsilon}\tilde{\epsilon}^\top}=R$, a symmetric positive definite matrix.

The maximum likelihood (ML) probability\footnote{The expectation value of eq.~\eqref{eq:LS-problem} is $N-p$. This can be seen using eq.~\eqref{eq:thetaLS-estimate}
  \begin{equation*}
    Y-\Phi\thetaLS = (\Phi\theta_0 + e) - \Phi\left(\theta_0 + K \Phi^\top e\right) = \left(I - \Phi K \Phi^\top\right)e.
  \end{equation*}
  The average
  \begin{equation*}
    \EE{\left|\left|Y - \Phi \thetaLS\right|\right|_2^2} = \EE{e^\top\left(I - \Phi K \Phi^\top\right)^\top\left(I - \Phi K \Phi^\top\right)e} = \EE{e^\top\left(I - \Phi K \Phi^\top\right)e}.
  \end{equation*}
  The term $\EE{e^\top e}$ evaluates to $N$. The other term evaluates to $p$: using $\Phi = U\Sigma V^\top$,
  \begin{equation*}
    (\Phi^\top \Phi)^{-1} = \left(V\Sigma^\top U^\top U\Sigma V^\top\right)^{-1} = V\left(\Sigma^\top\Sigma\right)^{-1}V^\top
  \end{equation*}
  and
  \begin{equation*}
    \Phi V\left(\Sigma^\top\Sigma\right)^{-1}V^\top \Phi^\top = U\Sigma \left(\Sigma^\top\Sigma\right)^{-1}\Sigma^\top U
  \end{equation*}
where the term between the two $U$ is a tall matrix of dimension $N\times p$ with ones on the top block's main diagonal (of dimension $p\times p$) and zeros in the bottom block.} requires us to minimize
\begin{equation}
  \label{eq:LS-problem}
  (\tilde{Y} - \tilde{\Phi} \theta)^\top R^{-1} (\tilde{Y} - \tilde{\Phi} \theta) = ||C^{-1} (\tilde{Y} - \tilde{\Phi} \theta)||_2^2 = ||Y - \Phi \theta||_2^2
\end{equation}
where $C$ is the Cholesky decomposition of $R = C C^\top$ (and $R^{-1} = \left(C^{-1}\right)^\top C^{-1}$), $Y = C^{-1}\tilde{Y}$, $\Phi = C^{-1}\tilde{\Phi}$ and $e=C^{-1}\tilde{\epsilon}$. Note that the scaled noise $e$ is uncorrelated and has unit variance:
\begin{equation*}
  \EE{e e^\top} = \EE{C^{-1}\tilde{\epsilon} \tilde{\epsilon}^\top \left(C^{-1}\right)^\top} = C^{-1}R \left(C^{-1}\right)^\top = \II.
\end{equation*}

The \emph{mathematical} solution to eq.~\eqref{eq:LS-problem} is found by setting the gradient of the expression with respect to $\theta$ to zero, which gives the normal equation
\begin{equation}
  \label{eq:LS-estimator}
  \left(\Phi^\top\Phi\right)^\top \thetaLS = \Phi^\top Y\ \rightarrow\ \thetaLS = \left(\Phi^\top \Phi\right)^{-1} \Phi^\top Y.
\end{equation}
The solution exists provided $\Phi$ has full column rank\footnote{The usual warning holds for the rank. Instead we want to have the matrix $\Phi$ with the smallest condition number for the estimate to be numerically stable, which is a stronger condition than full rank.}: $\rank{\Phi} = p$. When this is the case, the system is said to be \emph{persistently excited}, see Sect.. and when given the freedom, one chooses $\Phi$ so that it is well conditioned.

Had we not scaled $\tilde{Y}$ and $\tilde{\Phi}$ by the noise covariance, the solution would have been
\begin{equation*}
  \thetaLS = \left(\tilde{\Phi}^\top R^{-1}\tilde{\Phi}\right)^{-1} \tilde{\Phi}^\top R^{-1} \tilde{Y}.
\end{equation*}

\emph{Numerically} one should not form the normal equation directly because it squares the condition number of $\Phi$ and rely either on the QR decomposition or on the SVD to solve eq.~\eqref{eq:LS-problem}.
This is taken care automatically by MATLAB when using the backslash $\backslash$ operator
\begin{equation}
  \label{eq:LS-numerical-solution}
  \thetaLS = \Phi \backslash Y.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bias, Covariance and MSE of the Least Squares Estimation}
\label{sec:bias-variance-MSE-LS-estimation}

The linear estimator eq.~\eqref{eq:LS-estimator} is unbiased\footnote{Recalling that $\Phi^\top \Phi$ is a symmetric matrix, letting $K\doteq (\Phi^\top \Phi)^{-1}$ and using eq.~\eqref{eq:LTI-with-noise} into eq.~\eqref{eq:LS-estimator}, we obtain
  \begin{equation}
    \label{eq:thetaLS-estimate}
    \thetaLS = K\Phi^\top (\Phi\theta_0 + e) = \theta_0 + K\Phi^\top e
  \end{equation}
  from which $\EE{\thetaLS} = \theta_0$ since $\EE{e}=0$. Moreover
  \begin{align*}
    \cov{\thetaLS} &\doteq \EE{\left(\thetaLS - \EE{\thetaLS}\right) \left(\thetaLS - \EE{\thetaLS}\right)^\top} \\
                   &= \EE{\left(K \Phi^\top e\right) \left(K \Phi^\top e\right)^\top} \\
                   &= \EE{K \Phi^\top e e^\top \Phi K} = K \Phi^\top \EE{ee^\top} \Phi K = K.
  \end{align*}} (but see also Sect.~\ref{sec:estimation-bias}):
\begin{equation*}
  \EE{\thetaLS} = \theta_0.
\end{equation*}
The covariance\footnote{Is there a way to understand the form of covariance matrix without going through the calculation?}
\begin{equation}
  \label{eq:covariance-LS}
  \cov{\thetaLS} = \left(\Phi^\top \Phi\right)^{-1}.
\end{equation}
Lastly, we consider the mean squared error
\begin{equation*}
  \MSE{\thetaLS} = \underbrace{\left|\left|\Bias{\thetaLS}\right|\right|_2^2}_{=0} + \textrm{tr}\left(\cov{\thetaLS}\right)
\end{equation*}
which reduces to $N$ for the case of uncorrelated noise (I am not sure about this anymore).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Geometric Interpretation of Least-Squares}

The least squares problem
\begin{equation*}
  ||b - Ax||_2
\end{equation*}
has the following geometric interpretation: the solution is that for which the residuals $v \doteq b - Ax$ are outside (\text{i.e.} orthogonal) of the space spanned by $A$.
In other words, we require\footnote{I believe the proof given in class is not correct: to span the full column space of $A$, one has to multiply by a generic vector $z$; in class $z=x$ was taken.} the scalar product $\langle Az, v\rangle$ to be zero for all $z$:
\begin{align*}
  0 &= \langle Az, v\rangle = (Az)^\top(b-Ax) = z^\top \left(A^\top b - A^\top Ax\right)\hspace{2em}\forall z \\
    &\rightarrow A^\top Ax = A^\top b
\end{align*}
which is the normal equation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Random Notes: The Covariance Matrix and the Choice of the Measurement Points}
\label{question-covariance}

These are my considerations that are not part of the lecture.

\begin{itemize}
\item The \emph{off-diagonal} elements of the covariance matrix $\cov{\thetaLS}$ eq.~\eqref{eq:covariance-LS} represent the correlations between the errors of the variables $\theta$.
It is therefore not justified to discard them and take $\theta$'s standard deviations as the square root of $\cov{\theta}$'s diagonal elements because one discards the correlations: the ball of probability is in general an ellipse with the axes not parallel to the variable directions.

\item Given the freedom to choose the measurement points, is there a ``best'' way of placing them? Is this done in practice?

  In the context of experiment design, this is done.

  There are two factors that determine the covariance matrix: the choice of basis and the choice of points.
  The choice of basis is determined by the variables that one wants to extract: a linear transformation between one basis and the other will also transform the covariance matrix and the only concern may be the numerical stability (although one should expect that the measurement errors dominate).

  Determining the position of the best measurement points by minimizing one (or more elements of the covariance) is in general a non-convex problem.
\end{itemize}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "notes"
%%% End:
