\chapter{Regularized FIR Models}
\label{chap:regularised-FIR-models}

In Sect.~\ref{sec:bias-variance-MSE-LS-estimation}, we have seen that the solution
\begin{equation*}
  \thetaLS = \arg\min_\theta ||Y-\Phi \theta||_2^2
\end{equation*}
of the least squares problem is unbiased. We can however choose to have a biased estimate to reduce the mean square error $\EE{||\hat{\theta}-\theta_0||_2^2}$. This can be achieved if we modify the minimization problem by adding a regularization term
\begin{equation}
  \label{eq:regularized-LS}
  ||Y-\Phi \theta||_2^2 + \gamma \theta^\top P^{-1}\theta
\end{equation}
where $\gamma P^{-1}$ is a positive definite matrix. $P$ is called the \emph{kernel} or the regularization matrix\footnote{When $P=\II_N$ the technique is called ridge regression, otherwise it goes under the name of Tikhonov regularization. Compared to the ridge regression which only tries to decrease $||\theta||^2$, the Tikhonov regularization can use information about the system, see Sect.~\ref{sec:regularization-matrices}.}.

Regularization prevents overfitting, reduces the sensitivity to noise and can improve the estimate by a proper choice of the kernel (\textit{e.g.} for instance if the system is known to be stable, this information can be used to improve the estimation.)

% The model contains also a truncation error
%How to choose $\taumax$

The closed form mathematical solution to eq.~\eqref{eq:regularized-LS} is given by
\begin{equation}
  \label{eq:regularized-LS-solution}
  \thetaR = \left(\Phi^\top\Phi + \gamma P^{-1}\right)^{-1}\Phi^\top Y = \left(P\Phi^\top\Phi + \gamma\II \right)^{-1}P\Phi^\top Y.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The James-Stein Estimator}
\label{sec:james-stein-estimator}

For the problem $Y=\theta_o+e$, the James-Stein estimator is a biased estimator that has a smaller MSE\footnote{I am not sure whether the James-Stein estimator is obtained by minimizing the MSE. We have $\thetaJS = (1-r)\thetaLS = (1-r)(\theta_0+e)$; the MSE is
  \begin{align*}
    \EE{(\thetaJS-\theta_0)(\thetaJS-\theta_0)^\top} = (1-r)^2||\theta_0||_2^2 + r^2\underbrace{\EE{||e||_2^2}}_{=N\sigma^2}
  \end{align*}
  which has a minimum at
  \begin{equation*}
    r = \frac{N\sigma^2}{||\theta_0||^2+N\sigma^2}.
  \end{equation*}
  We also have the identity $\EE{||Y||_2^2} = ||\theta_0||_2^2 + N\sigma^2$ but the James-Stein estimator contains $||Y||_2^2$ at the denominator and not $\EE{||Y||_2^2}$.} than the least squares. This is achieved by ``shrinking the estimate towards the origin''~\cite[page~3]{pillonetto}:
\begin{equation*}
  \thetaJS = (1-r)\thetaLS,\hspace{2em} r\doteq \frac{(N-2)\sigma^2}{||Y||^2}.
\end{equation*}
where $N=p$. It can be cast into the ridge regression problem
\begin{equation*}
  ||Y-\theta||^2 + \gamma ||\theta||^2,\hspace{2em} \gamma = \frac{(N-2)\sigma^2}{||Y||^2 - (N-2)\sigma^2}.
\end{equation*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bias, Covariance and MSE of the Regularized Least Squares Estimation}
\label{sec:bias-covariance-MSE-regLS-estimation}

We assume uncorrelated noise with constant variance $1$: $\EE{e e^\top} = \II_N$.

The estimate eq.~\eqref{eq:regularized-LS-solution} has bias\footnote{Let $K = \left(\Phi^\top\Phi + \gamma P^{-1}\right)^{-1}$ so that $\thetaR = K\Phi^\top Y$. Plugging $Y=\Phi\theta_0+e$, we have $\EE{\thetaR} = K\Phi^\top\Phi\theta_0$. The bias is
  \begin{equation*}
    \EE{\thetaR} - \theta_0 = (K\Phi^\top\Phi - \II)\theta_0 = K(\Phi^\top\Phi - K^{-1})\theta_0 = -\gamma KP^{-1}\theta_0.
  \end{equation*}}, covariance\footnote{With the same notation as before, we have $\thetaR - \EE{\thetaR} = K\Phi^\top e$. The covariance is
  \begin{equation*}
    \cov{\thetaR} = \EE{\left(K\Phi^\top e\right) \left(K\Phi^\top e\right)^\top} = K\Phi^\top \EE{e e^\top} \Phi K = K\Phi^\top\Phi K.
  \end{equation*}} and MSE
\begin{align*}
  \Bias{\thetaR} &= -\left(\Phi^\top\Phi + \gamma P^{-1}\right)^{-1}\gamma P^{-1}\theta_0 \\
  \cov{\thetaR} &= \left(\Phi^\top\Phi + \gamma P^{-1}\right)^{-1} \Phi^\top\Phi \left(\Phi^\top\Phi + \gamma P^{-1}\right)^{-1} \\
  \MSE{\thetaR} &= \left|\left|\Bias{\thetaR}\right|\right|_2^2 + \tr{\cov{\thetaR}}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Choice of the Regularization Matrices}
\label{sec:regularization-matrices}

The MSE is minimized by this choice of parameters
\begin{equation}
  \label{eq:regLS-optimal-parameters}
  \gamma^\star = 1,\hspace{1em} P^\star = \theta_0^{\phantom{\top}} \theta_0^\top.
\end{equation}
The optimal regularization matrix $P^\star$ is unknown\footnote{\begin{quotation}
  $P^\star = \theta_0^{\phantom{\top}} \theta_0^\top$ has rank 1 and is therefore not invertible. To get a well-defined problem, the inverse $P^{-1}$ is replaced by the Moore-Penrose pseudoinverse $P^+$. It turns out that the solution of the problem defined with $P^\dagger$ is also equal to $(P\Phi^\top \Phi+\gamma\II)^{-1}P\Phi^\top Y$. [Mohamed Abdalmoaty on Moodle]
\end{quotation}} because it depends on the unknown $\theta_0$ but the approximate knowledge of the solution helps to construct a ``good'' $P$.

\begin{quotation}
  Regularization functions are designed to encode prior knowledge on the unknown system; some choices will result in better estimates. The choice can be either subjective or based on cross-validation and different regularization functions/kernels give rise to different model classes. Technically there is no wrong regularization function, but there is an optimal one (in the sense of minimum MSE).

  A quadratic regularization function based on the TC kernel is well-suited for exponentially decaying pulse responses of stable linear systems. In that case, a high order FIR model is used to estimate the first significant part of the pulse response of the unknown system, ignoring the tail. An FIR system is always stable, but its coefficients may not be exponentially decaying. An impulse response experiment may be used to reveal some properties of the system if the system and experimental configuration allow it. [Mohamed Abdalmoaty on Moodle]
  \end{quotation}

A commonly used regularization matrix is the diagonally-correlated (DC) kernel
\begin{equation*}
  [P]_{ij} = c\alpha^{\frac{i+j}{2}}\rho^{|i-j|}
\end{equation*}
with $0 \le c$, $0 \le \alpha \le 1$ and $-1 \le \rho \le 1$: $\rho$ describes correlations $\alpha$ and the decays.

A simpler version of it is the tuned-correlated (TC) kernel\footnote{Despite having entries that are exponential, the TC kernel is \emph{not} $\theta \theta^\top$
  \begin{equation*}
    \theta \theta^\top =
    \begin{bmatrix}
      a & a^2 & a^3 & \ldots \\
      a^2 & a^3 & \ldots \\
      a^3 & \ldots \\
      \ldots
    \end{bmatrix}\hspace{1em} \textrm{with } \theta =
    \begin{bmatrix}
      a & a^2 & \ldots
    \end{bmatrix}
  \end{equation*}
  coming from the truncation of the IIR filter $\tfrac{z^{-1}}{1-az^{-1}} \approx z^{-1}(1+az^{-1}+a^2z^{-2}+\ldots)$.} where one lets $\rho = \sqrt{\alpha}$ in the DC kernel
\begin{equation*}
  [P]_{ij} = c\alpha^{\max\{i,j\}} =
  \begin{bmatrix}
    \alpha   & \alpha^2 & \alpha^3 & \ldots & \alpha^n \\
    \alpha^2 & \alpha^2 & \alpha^3 & \ldots & \alpha^n \\
    \alpha^3 & \alpha^3 & \alpha^3 & \ldots & \alpha^n \\
    \ldots \\
    \alpha^n & \alpha^n & \alpha^n & \ldots & \alpha^n \\
  \end{bmatrix}
\end{equation*}
The top left entries are large, the bottom right small. Since the inverse enters the minimization problem (cost function), the top left part is small whereas the bottom down are large and induce a larger cost.

The inverse of the DC kernel is a tridiagonal with the explicit form
\begin{equation*}
  [P^{-1}]_{ij} = \frac{c_{ij}}{1-\rho^2}(-1)^{i+j}\alpha^{-\frac{i+j}{2}}\rho^{|i-j|},\hspace{2em} c_{ij} =
  \begin{cases}
    1 + \rho^2 & \textrm{if } i=j=2,\ldots,p-1 \\
    0 & \textrm{if } |i-j| > 1 \\
    1 & \textrm{otherwise}
  \end{cases}
\end{equation*}
% Write p^{-1} explicitly.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimation Bias}
\label{sec:estimation-bias}

\begin{quotation}
  One cannot say that the linear least-squares method is unbiased in general. There are three elements at play: the data set, the model used, and the estimation method. Least-squares is just the estimation method. To check if it is unbiased, an assumption has to be made on the true system that generated the data, and the model used to get the closed form expression of the least-squares estimator. [Mohamed Abdalmoaty on Moodle]
\end{quotation}

In the following, the input $u$ is known, the measurement noise $e$ has zero-mean, the data is generated as $Y=\Phi\theta_0+e$ with $\theta_0\in R^p$; we estimate $\theta\in R^q$ in the linear regression model $Y=\Phi_q\theta+e$; and we use the least-squares estimation method: then $\hat{\theta} = \left(\Phi_q^\top \Phi_q\right)^{-1} \Phi_q^\top Y$.

Here some examples of source of biased estimate:
\begin{itemize}
\item The model does not match the generating data. One underestimates the length of the response by choosing the model order too small, $q<p=\taumax$: this is the truncation error of Sect.. (see slide 10--10). Indeed we are trying to fit the data with a model of order $q$ but the model has order $p$. The estimate is biased because
  \begin{equation*}
    \EE{\hat{\theta}} = \left(\Phi_q^\top \Phi_q\right)^{-1}\Phi_q^\top\Phi \theta_0 \neq \theta_0.
  \end{equation*}
  This means the trade-off bias-variance can also be done with LS by varying the order parameter, but to a lower extent than using regularization where one has extra parameters available.

  Note that here the regressor involves only the known (noise-free) input $u$;
\item The data comes from an ARX system with $p=n+m$ parameters and $\theta_0\in R^{n+m}$ is the true parameter. As opposed to the case above, here $\Phi$ involves also the random outputs: for this reason the resulting LS estimator is \emph{biased} even though we have a correct model order/structure (\textit{i.e.}, the model that matches the data generating system);
\item Finally, even with the correct order parameter, one can \emph{on purpose} bias $\hat{\theta}$ by adding the regularization term to $||Y-\Phi\theta||_2$.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{Why the LS May Perform Badly}
\label{sec:LS-illconditioned}

Least-squares performs badly when the regression matrix $\Phi$ is ill-conditioned\footnote{The condition number of a matrix is defined as the ratio between the largest $\sigma_1$ and the smallest $\sigma_n$ singular value of the matrix
  \begin{equation*}
    \textrm{cond}(A) = \frac{\sigma_1}{\sigma_n}.
  \end{equation*}
  Note that from a numerical point of view, the matrix being full rank is not a guarantee that the LS problem can be stably solved.}, as a result of not being persistently excited and a small perturbation of the measurement data $Y$ can have a large impact on the estimate $\thetaLS$. Regularization helps in these cases. (Is there an easy way to show that $\Phi^\top\Phi + \gamma P^{-1}$ has a larger minimum singular value?)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Cross-Validation Methods}
\label{sec:cross-validation}

They are methods to select the discrete model orders (in least-squares or ARX) or the regularization parameters (\textit{e.g.} $\gamma$ and the kernel parameter $\alpha$ in regularized least squares).

A popular method is called hold-out cross-validation and consists of the following steps:
\begin{enumerate}
\item split the data into two (equal but not necessarily) non-overlapping parts, one for estimation and the other for validation. The way in which the data is split depends on the type of data: for data generated by dynamical systems, where the data is sequential in time, the order is important. Here the first $N_\textrm{id}$ pairs of control inputs and output values $\{u(k),y(k)\}$ are used for identification and the remaining for validation. For static data, the order is not important: one could choose the odd-indexed pairs for identification and the even-indexed pairs for validation;
\item use the estimation data to estimate a model for each mode structure or each regularization parameter. In the case of the continuous parameter $\gamma$, one estimates the model on a set of gridded values. If there are two continuous parameters, \textit{e.g.} $\gamma$ and $\alpha$, the gridding is done for both parameters;
\item select the model structure / regularization parameters that give a model with least prediction error on validation data;
\item use the selected model structure / regularization parameters and the \emph{complete} data set to estimate a final model.
\end{enumerate}

Numerical example in \texttt{10\_lect/regularization.m}, for the moment limited to ridge regression.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Numerical Solution of the Tikhonov Regularization Problem}
\label{sec:numerical-tikhonov}

For the numerical solution I am aware of three methods:
  \begin{enumerate}
  \item Solve directly the normal equation
    \begin{equation}
      \label{eq:regularized-normal-equation}
      \left(\Phi^\top\Phi + \gamma P^{-1}\right) \hat{\theta} = \Phi^\top y
    \end{equation} using MATLAB's backslash operator: although this squares $\Phi$'s condition number when constructing $\Phi^\top\Phi$, at least it does not require to explicitly construct the inverse;
  \item Use the generalized SVD decomposition.
    We first manipulate eq.~\eqref{eq:regularized-LS} to obtain
    \begin{equation*}
      ||y-\Phi \theta||_2^2 + ||D\theta||_2^2
    \end{equation*}
    by letting $D$ be the Cholesky decomposition of $\gamma P^{-1}$: that is $D^\top D = \gamma P^{-1}$.

    There exists different definitions of generalized SVD: MATLAB implements the call \texttt{[U,V,X,C,S] = gsvd($\Phi$, D)} where $U$ and $V$ are unitary matrices, and $C$ and $S$ are nonnegative diagonal matrices such that
    \begin{equation*}
      \Phi = UCX^\top\hspace{2em} D = VSX^\top\hspace{2em} C^2+S^2=I.
    \end{equation*}
    Inserting the decompositions into eq.~\eqref{eq:regularized-normal-equation} gives
    \begin{equation*}
      X^\top \theta = (UC)^\top y
    \end{equation*}
    which can be solved as $\theta = X^\top \backslash (UC)^\top y$;
  \item Eq.~\eqref{eq:regularized-LS} can also be rewritten as
    \begin{equation*}
      \left|\left|
          \begin{bmatrix}
            \Phi \\ D
          \end{bmatrix}\theta -
          \begin{bmatrix}
            y \\ 0
          \end{bmatrix}\right|\right|_2^2
    \end{equation*}
    and solved by backslash.

    I tested the three approaches on prob.~8 and found the fastest to be the third which does not construct the normal equation and is therefore better conditioned at the small cost of increasing the problem size by the $r$ rows of $P^{-1}$. The first method is twice slower and the second is 100 (!) times slower: the problem is most likely the call to \texttt{gsvd}. This is strange because this is the method normally suggested, but probably only for larger regularization matrices.
  \end{enumerate}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "notes"
%%% End:
